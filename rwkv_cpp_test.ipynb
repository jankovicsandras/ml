{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOdPL31MOL56uHHiOpPYtWs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jankovicsandras/ml/blob/main/rwkv_cpp_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Info\n",
        "\n",
        "https://github.com/BlinkDL/RWKV-LM\n",
        "\n",
        "https://github.com/saharNooby/rwkv.cpp\n",
        "\n",
        "https://huggingface.co/BlinkDL\n",
        "\n",
        "https://huggingface.co/BlinkDL/rwkv-4-raven\n",
        "\n",
        "https://huggingface.co/BlinkDL/rwkv-4-world/tree/main"
      ],
      "metadata": {
        "id": "Tq05i9d8qi8-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check if model file is downloaded"
      ],
      "metadata": {
        "id": "yIfIqQEDPnXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! ls -la\n",
        "! pwd\n",
        "! ls -la rwkv.cpp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhTEY8T3PmFU",
        "outputId": "2e1b3a2d-384a-47e5-b3e8-de928f337f9c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 11965804\n",
            "drwxr-xr-x  1 root root       4096 Oct 31 13:06 .\n",
            "drwxr-xr-x  1 root root       4096 Oct 31 13:00 ..\n",
            "drwxr-xr-x  4 root root       4096 Oct 27 13:22 .config\n",
            "-rw-r--r--  1 root root 6127353277 Oct 31 13:06 RWKV-4-World-3B-v1-20230619-ctx4096.bin\n",
            "-rw-r--r--  1 root root 6125597618 Jun 20 03:00 RWKV-4-World-3B-v1-20230619-ctx4096.pth\n",
            "drwxr-xr-x 11 root root       4096 Oct 31 13:01 rwkv.cpp\n",
            "drwxr-xr-x  1 root root       4096 Oct 27 13:22 sample_data\n",
            "/content\n",
            "total 652\n",
            "drwxr-xr-x 11 root root   4096 Oct 31 13:01 .\n",
            "drwxr-xr-x  1 root root   4096 Oct 31 13:06 ..\n",
            "drwxr-xr-x  2 root root   4096 Oct 31 13:01 bin\n",
            "-rw-r--r--  1 root root  15465 Oct 31 13:01 CMakeCache.txt\n",
            "drwxr-xr-x  7 root root   4096 Oct 31 13:01 CMakeFiles\n",
            "-rw-r--r--  1 root root   1823 Oct 31 13:01 cmake_install.cmake\n",
            "-rw-r--r--  1 root root  15316 Oct 31 13:01 CMakeLists.txt\n",
            "-rw-r--r--  1 root root   6962 Oct 31 13:01 compile_commands.json\n",
            "-rw-r--r--  1 root root    282 Oct 31 13:01 CTestTestfile.cmake\n",
            "drwxr-xr-x  2 root root   4096 Oct 31 13:01 docs\n",
            "drwxr-xr-x  3 root root   4096 Oct 31 13:01 extras\n",
            "drwxr-xr-x 10 root root   4096 Oct 31 13:01 ggml\n",
            "drwxr-xr-x  9 root root   4096 Oct 31 13:01 .git\n",
            "drwxr-xr-x  3 root root   4096 Oct 31 13:01 .github\n",
            "-rw-r--r--  1 root root    305 Oct 31 13:01 .gitignore\n",
            "-rw-r--r--  1 root root    116 Oct 31 13:01 .gitmodules\n",
            "-rwxr-xr-x  1 root root 435256 Oct 31 13:01 librwkv.so\n",
            "-rw-r--r--  1 root root   1067 Oct 31 13:01 LICENSE\n",
            "-rw-r--r--  1 root root  12895 Oct 31 13:01 Makefile\n",
            "drwxr-xr-x  4 root root   4096 Oct 31 13:01 python\n",
            "-rw-r--r--  1 root root  10629 Oct 31 13:01 README.md\n",
            "-rw-r--r--  1 root root   5855 Oct 31 13:01 rwkv.cpp\n",
            "-rw-r--r--  1 root root   3995 Oct 31 13:01 rwkv_error_handling.inc\n",
            "-rw-r--r--  1 root root   6409 Oct 31 13:01 rwkv_eval.inc\n",
            "-rw-r--r--  1 root root   7434 Oct 31 13:01 rwkv_file_format.inc\n",
            "-rw-r--r--  1 root root   1575 Oct 31 13:01 rwkv_gpu_offload.inc\n",
            "-rw-r--r--  1 root root  19906 Oct 31 13:01 rwkv_graph.inc\n",
            "-rw-r--r--  1 root root  12132 Oct 31 13:01 rwkv.h\n",
            "-rw-r--r--  1 root root   7354 Oct 31 13:01 rwkv_model_loading.inc\n",
            "-rw-r--r--  1 root root   4155 Oct 31 13:01 rwkv_operators.inc\n",
            "-rw-r--r--  1 root root   6880 Oct 31 13:01 rwkv_quantize.inc\n",
            "-rw-r--r--  1 root root   1857 Oct 31 13:01 rwkv_utilities.inc\n",
            "drwxr-xr-x  3 root root   4096 Oct 31 13:01 tests\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# rwkv.cpp download and build"
      ],
      "metadata": {
        "id": "337frCuWQC42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone --recursive https://github.com/saharNooby/rwkv.cpp.git\n",
        "! cd rwkv.cpp && cmake . && cmake --build . --config Release"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NTifmRqP4Rr",
        "outputId": "acc1c42d-7ea7-4f48-cb50-2bc5e0fd6c21"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'rwkv.cpp'...\n",
            "remote: Enumerating objects: 2013, done.\u001b[K\n",
            "remote: Counting objects: 100% (665/665), done.\u001b[K\n",
            "remote: Compressing objects: 100% (240/240), done.\u001b[K\n",
            "remote: Total 2013 (delta 493), reused 523 (delta 407), pack-reused 1348\u001b[K\n",
            "Receiving objects: 100% (2013/2013), 6.76 MiB | 22.77 MiB/s, done.\n",
            "Resolving deltas: 100% (1241/1241), done.\n",
            "Submodule 'ggml' (https://github.com/saharNooby/ggml) registered for path 'ggml'\n",
            "Cloning into '/content/rwkv.cpp/ggml'...\n",
            "remote: Enumerating objects: 2951, done.        \n",
            "remote: Counting objects: 100% (1345/1345), done.        \n",
            "remote: Compressing objects: 100% (174/174), done.        \n",
            "remote: Total 2951 (delta 1238), reused 1171 (delta 1171), pack-reused 1606        \n",
            "Receiving objects: 100% (2951/2951), 4.94 MiB | 18.80 MiB/s, done.\n",
            "Resolving deltas: 100% (1994/1994), done.\n",
            "Submodule path 'ggml': checked out 'd925ed7a96767192d422a97645f08ad86d5cc6f0'\n",
            "-- The C compiler identification is GNU 11.4.0\n",
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /usr/bin/cc - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "-- Found Threads: TRUE  \n",
            "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "-- x86 detected\n",
            "-- Configuring done (0.9s)\n",
            "-- Generating done (0.0s)\n",
            "-- Build files have been written to: /content/rwkv.cpp\n",
            "[  4%] \u001b[32mBuilding C object CMakeFiles/ggml.dir/ggml/src/ggml.c.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding C object CMakeFiles/ggml.dir/ggml/src/ggml-alloc.c.o\u001b[0m\n",
            "[  9%] Built target ggml\n",
            "[ 13%] \u001b[32mBuilding CXX object CMakeFiles/rwkv.dir/rwkv.cpp.o\u001b[0m\n",
            "In file included from \u001b[01m\u001b[K/content/rwkv.cpp/rwkv.cpp:42\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/rwkv.cpp/rwkv_utilities.inc:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kbool rwkv_fread_string(FILE*, size_t, std::string&)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/rwkv.cpp/rwkv_utilities.inc:36:18:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcast from type ‘\u001b[01m\u001b[Kconst char*\u001b[m\u001b[K’ to type ‘\u001b[01m\u001b[Kvoid*\u001b[m\u001b[K’ casts away qualifiers [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wcast-qual\u0007-Wcast-qual\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   36 |     return fread(\u001b[01;35m\u001b[K(void *) dest.data()\u001b[m\u001b[K, length, 1, file) == 1;\n",
            "      |                  \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/content/rwkv.cpp/rwkv.cpp:89\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/rwkv.cpp/rwkv_gpu_offload.inc:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kbool rwkv_gpu_offload_layers(rwkv_context*, uint32_t)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/rwkv.cpp/rwkv_gpu_offload.inc:54:52:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused parameter ‘\u001b[01m\u001b[Kctx\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-parameter\u0007-Wunused-parameter\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   54 | bool rwkv_gpu_offload_layers(\u001b[01;35m\u001b[Kstruct rwkv_context * ctx\u001b[m\u001b[K, const uint32_t n_layers) {\n",
            "      |                              \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~^~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/rwkv.cpp/rwkv_gpu_offload.inc:54:72:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused parameter ‘\u001b[01m\u001b[Kn_layers\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-parameter\u0007-Wunused-parameter\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   54 | bool rwkv_gpu_offload_layers(struct rwkv_context * ctx, \u001b[01;35m\u001b[Kconst uint32_t n_layers\u001b[m\u001b[K) {\n",
            "      |                                                         \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~^~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/content/rwkv.cpp/rwkv.cpp:91\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/rwkv.cpp/rwkv_eval.inc:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kbool rwkv_eval_sequence_in_chunks(rwkv_context*, const uint32_t*, size_t, size_t, const float*, float*, float*)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/rwkv.cpp/rwkv_eval.inc:135:32:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcast from type ‘\u001b[01m\u001b[Kconst uint32_t*\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Kconst unsigned int*\u001b[m\u001b[K’} to type ‘\u001b[01m\u001b[Kuint32_t*\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Kunsigned int*\u001b[m\u001b[K’} casts away qualifiers [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wcast-qual\u0007-Wcast-qual\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  135 |     uint32_t * tokens_offset = \u001b[01;35m\u001b[K(uint32_t *) tokens\u001b[m\u001b[K;\n",
            "      |                                \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "[ 18%] \u001b[32m\u001b[1mLinking CXX shared library librwkv.so\u001b[0m\n",
            "[ 18%] Built target rwkv\n",
            "[ 22%] \u001b[32mBuilding C object tests/CMakeFiles/test_ggml_basics.dir/test_ggml_basics.c.o\u001b[0m\n",
            "[ 27%] \u001b[32m\u001b[1mLinking C executable ../bin/test_ggml_basics\u001b[0m\n",
            "[ 27%] Built target test_ggml_basics\n",
            "[ 31%] \u001b[32mBuilding C object tests/CMakeFiles/test_quantized_matmul_on_gpu.dir/test_quantized_matmul_on_gpu.c.o\u001b[0m\n",
            "[ 36%] \u001b[32m\u001b[1mLinking C executable ../bin/test_quantized_matmul_on_gpu\u001b[0m\n",
            "[ 36%] Built target test_quantized_matmul_on_gpu\n",
            "[ 40%] \u001b[32mBuilding C object tests/CMakeFiles/test_tiny_rwkv.dir/test_tiny_rwkv.c.o\u001b[0m\n",
            "In file included from \u001b[01m\u001b[K/content/rwkv.cpp/tests/test_tiny_rwkv.c:7\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/rwkv.cpp/tests/logit_difference_validator.inc:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Ktest_model\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/rwkv.cpp/tests/logit_difference_validator.inc:63:52:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kimplicit conversion from ‘\u001b[01m\u001b[Kfloat\u001b[m\u001b[K’ to ‘\u001b[01m\u001b[Kdouble\u001b[m\u001b[K’ when passing argument to function [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdouble-promotion\u0007-Wdouble-promotion\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   63 |     fprintf(stderr, \"Serial difference sum: %f\\n\", \u001b[01;35m\u001b[Kdiff_sum\u001b[m\u001b[K);\n",
            "      |                                                    \u001b[01;35m\u001b[K^~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/content/rwkv.cpp/tests/test_tiny_rwkv.c:7\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/rwkv.cpp/tests/logit_difference_validator.inc:79:54:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kimplicit conversion from ‘\u001b[01m\u001b[Kfloat\u001b[m\u001b[K’ to ‘\u001b[01m\u001b[Kdouble\u001b[m\u001b[K’ when passing argument to function [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdouble-promotion\u0007-Wdouble-promotion\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   79 |     fprintf(stderr, \"Sequence difference sum: %f\\n\", \u001b[01;35m\u001b[Kdiff_sum\u001b[m\u001b[K);\n",
            "      |                                                      \u001b[01;35m\u001b[K^~~~~~~~\u001b[m\u001b[K\n",
            "[ 45%] \u001b[32m\u001b[1mLinking C executable ../bin/test_tiny_rwkv\u001b[0m\n",
            "[ 45%] Built target test_tiny_rwkv\n",
            "[ 50%] \u001b[32mBuilding C object tests/CMakeFiles/test_quantization_format_compatibility.dir/test_quantization_format_compatibility.c.o\u001b[0m\n",
            "In file included from \u001b[01m\u001b[K/content/rwkv.cpp/tests/test_quantization_format_compatibility.c:7\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/rwkv.cpp/tests/logit_difference_validator.inc:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Ktest_model\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/rwkv.cpp/tests/logit_difference_validator.inc:63:52:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kimplicit conversion from ‘\u001b[01m\u001b[Kfloat\u001b[m\u001b[K’ to ‘\u001b[01m\u001b[Kdouble\u001b[m\u001b[K’ when passing argument to function [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdouble-promotion\u0007-Wdouble-promotion\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   63 |     fprintf(stderr, \"Serial difference sum: %f\\n\", \u001b[01;35m\u001b[Kdiff_sum\u001b[m\u001b[K);\n",
            "      |                                                    \u001b[01;35m\u001b[K^~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/content/rwkv.cpp/tests/test_quantization_format_compatibility.c:7\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/rwkv.cpp/tests/logit_difference_validator.inc:79:54:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kimplicit conversion from ‘\u001b[01m\u001b[Kfloat\u001b[m\u001b[K’ to ‘\u001b[01m\u001b[Kdouble\u001b[m\u001b[K’ when passing argument to function [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wdouble-promotion\u0007-Wdouble-promotion\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   79 |     fprintf(stderr, \"Sequence difference sum: %f\\n\", \u001b[01;35m\u001b[Kdiff_sum\u001b[m\u001b[K);\n",
            "      |                                                      \u001b[01;35m\u001b[K^~~~~~~~\u001b[m\u001b[K\n",
            "[ 54%] \u001b[32m\u001b[1mLinking C executable ../bin/test_quantization_format_compatibility\u001b[0m\n",
            "[ 54%] Built target test_quantization_format_compatibility\n",
            "[ 59%] \u001b[32mBuilding C object tests/CMakeFiles/test_logit_calculation_skipping.dir/test_logit_calculation_skipping.c.o\u001b[0m\n",
            "[ 63%] \u001b[32m\u001b[1mLinking C executable ../bin/test_logit_calculation_skipping\u001b[0m\n",
            "[ 63%] Built target test_logit_calculation_skipping\n",
            "[ 68%] \u001b[32mBuilding C object tests/CMakeFiles/test_eval_sequence_in_chunks.dir/test_eval_sequence_in_chunks.c.o\u001b[0m\n",
            "\u001b[01m\u001b[K/content/rwkv.cpp/tests/test_eval_sequence_in_chunks.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Ktest_on_prompt\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/rwkv.cpp/tests/test_eval_sequence_in_chunks.c:33:23:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison of integer expressions of different signedness: ‘\u001b[01m\u001b[Kint\u001b[m\u001b[K’ and ‘\u001b[01m\u001b[Ksize_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wsign-compare\u0007-Wsign-compare\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   33 |     for (int i = 0; i \u001b[01;35m\u001b[K<\u001b[m\u001b[K prompt_length; i++) {\n",
            "      |                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "[ 72%] \u001b[32m\u001b[1mLinking C executable ../bin/test_eval_sequence_in_chunks\u001b[0m\n",
            "[ 72%] Built target test_eval_sequence_in_chunks\n",
            "[ 77%] \u001b[32mBuilding C object tests/CMakeFiles/test_context_cloning.dir/test_context_cloning.c.o\u001b[0m\n",
            "[ 81%] \u001b[32m\u001b[1mLinking C executable ../bin/test_context_cloning\u001b[0m\n",
            "[ 81%] Built target test_context_cloning\n",
            "[ 86%] \u001b[32mBuilding C object extras/CMakeFiles/rwkv_cpu_info.dir/cpu_info.c.o\u001b[0m\n",
            "[ 90%] \u001b[32m\u001b[1mLinking C executable ../bin/rwkv_cpu_info\u001b[0m\n",
            "[ 90%] Built target rwkv_cpu_info\n",
            "[ 95%] \u001b[32mBuilding C object extras/CMakeFiles/rwkv_quantize.dir/quantize.c.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking C executable ../bin/rwkv_quantize\u001b[0m\n",
            "[100%] Built target rwkv_quantize\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download model"
      ],
      "metadata": {
        "id": "z1cEvUsgxQ91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! wget https://huggingface.co/BlinkDL/rwkv-4-world/resolve/main/RWKV-4-World-3B-v1-20230619-ctx4096.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdeGyqbToFAc",
        "outputId": "9d8c00f7-86c8-444d-dc9c-99b99d283543"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-31 13:01:52--  https://huggingface.co/BlinkDL/rwkv-4-world/resolve/main/RWKV-4-World-3B-v1-20230619-ctx4096.pth\n",
            "Resolving huggingface.co (huggingface.co)... 18.172.134.4, 18.172.134.24, 18.172.134.88, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.172.134.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/7f/9c/7f9cfb689f066fd31f71b817e907c34f0b02f23e488bf7de8c808191cba731f0/1b227af317fa25b6939ab3c7cd321226ca48b8fe4bbbd2df3db669f1482c54ba?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27RWKV-4-World-3B-v1-20230619-ctx4096.pth%3B+filename%3D%22RWKV-4-World-3B-v1-20230619-ctx4096.pth%22%3B&Expires=1699016512&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5OTAxNjUxMn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy83Zi85Yy83ZjljZmI2ODlmMDY2ZmQzMWY3MWI4MTdlOTA3YzM0ZjBiMDJmMjNlNDg4YmY3ZGU4YzgwODE5MWNiYTczMWYwLzFiMjI3YWYzMTdmYTI1YjY5MzlhYjNjN2NkMzIxMjI2Y2E0OGI4ZmU0YmJiZDJkZjNkYjY2OWYxNDgyYzU0YmE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=conKq6mT2vDoBKEcHz7y-wQscqJjEQqSLE54v2VHbvW8HWwMvnpjhIrQ8snktTz2Snu0SHb3uY1sSKuYp8JWWs8l53SPKMMU60vn%7Ezs0uksfsglHs%7Eyw%7EyvHcO7ZW70i3eQXsgeGkTc2dg9VMS8mC1ffYAMe4Ax-lnLJQqTQEXxJVQm-XVJwPzfAF9NZNGhFHxmk%7EUJY7CsW2rV2l2GQTEPV-tlJXM8fP034BbvFgzXJDcWTke-2lhA0Ya1pPtDxLy7PG2YaAWDRLohIJno9TMI-oMapurTfF8frwdwhxrwOJjxPgpcKlKxQCPxzjcNPVvppkQA0FCrlV0foNdVwKQ__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-10-31 13:01:52--  https://cdn-lfs.huggingface.co/repos/7f/9c/7f9cfb689f066fd31f71b817e907c34f0b02f23e488bf7de8c808191cba731f0/1b227af317fa25b6939ab3c7cd321226ca48b8fe4bbbd2df3db669f1482c54ba?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27RWKV-4-World-3B-v1-20230619-ctx4096.pth%3B+filename%3D%22RWKV-4-World-3B-v1-20230619-ctx4096.pth%22%3B&Expires=1699016512&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5OTAxNjUxMn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy83Zi85Yy83ZjljZmI2ODlmMDY2ZmQzMWY3MWI4MTdlOTA3YzM0ZjBiMDJmMjNlNDg4YmY3ZGU4YzgwODE5MWNiYTczMWYwLzFiMjI3YWYzMTdmYTI1YjY5MzlhYjNjN2NkMzIxMjI2Y2E0OGI4ZmU0YmJiZDJkZjNkYjY2OWYxNDgyYzU0YmE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=conKq6mT2vDoBKEcHz7y-wQscqJjEQqSLE54v2VHbvW8HWwMvnpjhIrQ8snktTz2Snu0SHb3uY1sSKuYp8JWWs8l53SPKMMU60vn%7Ezs0uksfsglHs%7Eyw%7EyvHcO7ZW70i3eQXsgeGkTc2dg9VMS8mC1ffYAMe4Ax-lnLJQqTQEXxJVQm-XVJwPzfAF9NZNGhFHxmk%7EUJY7CsW2rV2l2GQTEPV-tlJXM8fP034BbvFgzXJDcWTke-2lhA0Ya1pPtDxLy7PG2YaAWDRLohIJno9TMI-oMapurTfF8frwdwhxrwOJjxPgpcKlKxQCPxzjcNPVvppkQA0FCrlV0foNdVwKQ__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.154.185.94, 18.154.185.26, 18.154.185.64, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.154.185.94|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6125597618 (5.7G) [binary/octet-stream]\n",
            "Saving to: ‘RWKV-4-World-3B-v1-20230619-ctx4096.pth’\n",
            "\n",
            "RWKV-4-World-3B-v1- 100%[===================>]   5.70G  22.5MB/s    in 3m 23s  \n",
            "\n",
            "2023-10-31 13:05:16 (28.8 MB/s) - ‘RWKV-4-World-3B-v1-20230619-ctx4096.pth’ saved [6125597618/6125597618]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Converting from pth to bin"
      ],
      "metadata": {
        "id": "9YPgx27E114D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python rwkv.cpp/python/convert_pytorch_to_ggml.py RWKV-4-World-3B-v1-20230619-ctx4096.pth RWKV-4-World-3B-v1-20230619-ctx4096.bin FP16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzudmGh7xYar",
        "outputId": "63ace051-31d0-4c96-83bd-3fb17065e493"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading RWKV-4-World-3B-v1-20230619-ctx4096.pth\n",
            "Writing emb.weight, shape torch.Size([65536, 2560]), type torch.float16\n",
            "Writing blocks.0.ln1.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.0.ln1.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.0.ln2.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.0.ln2.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.0.ln0.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.0.ln0.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.0.att.time_decay, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.0.att.time_first, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.0.att.time_mix_k, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.0.att.time_mix_v, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.0.att.time_mix_r, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.0.att.key.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.0.att.value.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.0.att.receptance.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.0.att.output.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.0.ffn.time_mix_k, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.0.ffn.time_mix_r, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.0.ffn.key.weight, shape torch.Size([10240, 2560]), type torch.float16\n",
            "Writing blocks.0.ffn.receptance.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.0.ffn.value.weight, shape torch.Size([2560, 10240]), type torch.float16\n",
            "Writing blocks.1.ln1.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.1.ln1.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.1.ln2.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.1.ln2.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.1.att.time_decay, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.1.att.time_first, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.1.att.time_mix_k, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.1.att.time_mix_v, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.1.att.time_mix_r, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.1.att.key.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.1.att.value.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.1.att.receptance.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.1.att.output.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.1.ffn.time_mix_k, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.1.ffn.time_mix_r, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.1.ffn.key.weight, shape torch.Size([10240, 2560]), type torch.float16\n",
            "Writing blocks.1.ffn.receptance.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.1.ffn.value.weight, shape torch.Size([2560, 10240]), type torch.float16\n",
            "Writing blocks.2.ln1.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.2.ln1.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.2.ln2.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.2.ln2.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.2.att.time_decay, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.2.att.time_first, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.2.att.time_mix_k, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.2.att.time_mix_v, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.2.att.time_mix_r, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.2.att.key.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.2.att.value.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.2.att.receptance.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.2.att.output.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.2.ffn.time_mix_k, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.2.ffn.time_mix_r, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.2.ffn.key.weight, shape torch.Size([10240, 2560]), type torch.float16\n",
            "Writing blocks.2.ffn.receptance.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.2.ffn.value.weight, shape torch.Size([2560, 10240]), type torch.float16\n",
            "Writing blocks.3.ln1.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.3.ln1.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.3.ln2.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.3.ln2.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.3.att.time_decay, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.3.att.time_first, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.3.att.time_mix_k, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.3.att.time_mix_v, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.3.att.time_mix_r, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.3.att.key.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.3.att.value.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.3.att.receptance.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.3.att.output.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.3.ffn.time_mix_k, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.3.ffn.time_mix_r, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.3.ffn.key.weight, shape torch.Size([10240, 2560]), type torch.float16\n",
            "Writing blocks.3.ffn.receptance.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.3.ffn.value.weight, shape torch.Size([2560, 10240]), type torch.float16\n",
            "Writing blocks.4.ln1.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.4.ln1.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.4.ln2.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.4.ln2.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.4.att.time_decay, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.4.att.time_first, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.4.att.time_mix_k, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.4.att.time_mix_v, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.4.att.time_mix_r, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.4.att.key.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.4.att.value.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.4.att.receptance.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.4.att.output.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.4.ffn.time_mix_k, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.4.ffn.time_mix_r, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.4.ffn.key.weight, shape torch.Size([10240, 2560]), type torch.float16\n",
            "Writing blocks.4.ffn.receptance.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.4.ffn.value.weight, shape torch.Size([2560, 10240]), type torch.float16\n",
            "Writing blocks.5.ln1.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.5.ln1.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.5.ln2.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.5.ln2.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.5.att.time_decay, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.5.att.time_first, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.5.att.time_mix_k, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.5.att.time_mix_v, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.5.att.time_mix_r, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.5.att.key.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.5.att.value.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.5.att.receptance.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.5.att.output.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.5.ffn.time_mix_k, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.5.ffn.time_mix_r, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.5.ffn.key.weight, shape torch.Size([10240, 2560]), type torch.float16\n",
            "Writing blocks.5.ffn.receptance.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.5.ffn.value.weight, shape torch.Size([2560, 10240]), type torch.float16\n",
            "Writing blocks.6.ln1.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.6.ln1.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.6.ln2.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.6.ln2.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.6.att.time_decay, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.6.att.time_first, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.6.att.time_mix_k, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.6.att.time_mix_v, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.6.att.time_mix_r, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.6.att.key.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.6.att.value.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.6.att.receptance.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.6.att.output.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.6.ffn.time_mix_k, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.6.ffn.time_mix_r, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.6.ffn.key.weight, shape torch.Size([10240, 2560]), type torch.float16\n",
            "Writing blocks.6.ffn.receptance.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.6.ffn.value.weight, shape torch.Size([2560, 10240]), type torch.float16\n",
            "Writing blocks.7.ln1.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.7.ln1.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.7.ln2.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.7.ln2.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.7.att.time_decay, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.7.att.time_first, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.7.att.time_mix_k, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.7.att.time_mix_v, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.7.att.time_mix_r, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.7.att.key.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.7.att.value.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.7.att.receptance.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.7.att.output.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.7.ffn.time_mix_k, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.7.ffn.time_mix_r, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.7.ffn.key.weight, shape torch.Size([10240, 2560]), type torch.float16\n",
            "Writing blocks.7.ffn.receptance.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.7.ffn.value.weight, shape torch.Size([2560, 10240]), type torch.float16\n",
            "Writing blocks.8.ln1.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.8.ln1.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.8.ln2.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.8.ln2.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.8.att.time_decay, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.8.att.time_first, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.8.att.time_mix_k, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.8.att.time_mix_v, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.8.att.time_mix_r, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.8.att.key.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.8.att.value.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.8.att.receptance.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.8.att.output.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.8.ffn.time_mix_k, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.8.ffn.time_mix_r, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.8.ffn.key.weight, shape torch.Size([10240, 2560]), type torch.float16\n",
            "Writing blocks.8.ffn.receptance.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.8.ffn.value.weight, shape torch.Size([2560, 10240]), type torch.float16\n",
            "Writing blocks.9.ln1.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.9.ln1.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.9.ln2.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.9.ln2.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.9.att.time_decay, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.9.att.time_first, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.9.att.time_mix_k, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.9.att.time_mix_v, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.9.att.time_mix_r, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.9.att.key.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.9.att.value.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.9.att.receptance.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.9.att.output.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.9.ffn.time_mix_k, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.9.ffn.time_mix_r, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.9.ffn.key.weight, shape torch.Size([10240, 2560]), type torch.float16\n",
            "Writing blocks.9.ffn.receptance.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.9.ffn.value.weight, shape torch.Size([2560, 10240]), type torch.float16\n",
            "Writing blocks.10.ln1.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.10.ln1.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.10.ln2.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.10.ln2.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.10.att.time_decay, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.10.att.time_first, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.10.att.time_mix_k, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.10.att.time_mix_v, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.10.att.time_mix_r, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.10.att.key.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.10.att.value.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.10.att.receptance.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.10.att.output.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.10.ffn.time_mix_k, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.10.ffn.time_mix_r, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.10.ffn.key.weight, shape torch.Size([10240, 2560]), type torch.float16\n",
            "Writing blocks.10.ffn.receptance.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.10.ffn.value.weight, shape torch.Size([2560, 10240]), type torch.float16\n",
            "Writing blocks.11.ln1.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.11.ln1.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.11.ln2.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.11.ln2.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.11.att.time_decay, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.11.att.time_first, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.11.att.time_mix_k, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.11.att.time_mix_v, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.11.att.time_mix_r, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.11.att.key.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.11.att.value.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.11.att.receptance.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.11.att.output.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.11.ffn.time_mix_k, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.11.ffn.time_mix_r, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.11.ffn.key.weight, shape torch.Size([10240, 2560]), type torch.float16\n",
            "Writing blocks.11.ffn.receptance.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.11.ffn.value.weight, shape torch.Size([2560, 10240]), type torch.float16\n",
            "Writing blocks.12.ln1.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.12.ln1.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.12.ln2.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.12.ln2.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.12.att.time_decay, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.12.att.time_first, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.12.att.time_mix_k, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.12.att.time_mix_v, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.12.att.time_mix_r, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.12.att.key.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.12.att.value.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.12.att.receptance.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.12.att.output.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.12.ffn.time_mix_k, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.12.ffn.time_mix_r, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.12.ffn.key.weight, shape torch.Size([10240, 2560]), type torch.float16\n",
            "Writing blocks.12.ffn.receptance.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.12.ffn.value.weight, shape torch.Size([2560, 10240]), type torch.float16\n",
            "Writing blocks.13.ln1.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.13.ln1.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.13.ln2.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.13.ln2.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.13.att.time_decay, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.13.att.time_first, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.13.att.time_mix_k, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.13.att.time_mix_v, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.13.att.time_mix_r, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.13.att.key.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.13.att.value.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.13.att.receptance.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.13.att.output.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.13.ffn.time_mix_k, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.13.ffn.time_mix_r, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.13.ffn.key.weight, shape torch.Size([10240, 2560]), type torch.float16\n",
            "Writing blocks.13.ffn.receptance.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.13.ffn.value.weight, shape torch.Size([2560, 10240]), type torch.float16\n",
            "Writing blocks.14.ln1.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.14.ln1.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.14.ln2.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.14.ln2.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.14.att.time_decay, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.14.att.time_first, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.14.att.time_mix_k, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.14.att.time_mix_v, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.14.att.time_mix_r, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.14.att.key.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.14.att.value.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.14.att.receptance.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.14.att.output.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.14.ffn.time_mix_k, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.14.ffn.time_mix_r, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.14.ffn.key.weight, shape torch.Size([10240, 2560]), type torch.float16\n",
            "Writing blocks.14.ffn.receptance.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.14.ffn.value.weight, shape torch.Size([2560, 10240]), type torch.float16\n",
            "Writing blocks.15.ln1.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.15.ln1.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.15.ln2.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.15.ln2.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.15.att.time_decay, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.15.att.time_first, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.15.att.time_mix_k, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.15.att.time_mix_v, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.15.att.time_mix_r, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.15.att.key.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.15.att.value.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.15.att.receptance.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.15.att.output.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.15.ffn.time_mix_k, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.15.ffn.time_mix_r, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.15.ffn.key.weight, shape torch.Size([10240, 2560]), type torch.float16\n",
            "Writing blocks.15.ffn.receptance.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.15.ffn.value.weight, shape torch.Size([2560, 10240]), type torch.float16\n",
            "Writing blocks.16.ln1.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.16.ln1.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.16.ln2.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.16.ln2.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.16.att.time_decay, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.16.att.time_first, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.16.att.time_mix_k, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.16.att.time_mix_v, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.16.att.time_mix_r, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.16.att.key.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.16.att.value.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.16.att.receptance.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.16.att.output.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.16.ffn.time_mix_k, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.16.ffn.time_mix_r, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.16.ffn.key.weight, shape torch.Size([10240, 2560]), type torch.float16\n",
            "Writing blocks.16.ffn.receptance.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.16.ffn.value.weight, shape torch.Size([2560, 10240]), type torch.float16\n",
            "Writing blocks.17.ln1.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.17.ln1.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.17.ln2.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.17.ln2.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.17.att.time_decay, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.17.att.time_first, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.17.att.time_mix_k, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.17.att.time_mix_v, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.17.att.time_mix_r, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.17.att.key.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.17.att.value.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.17.att.receptance.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.17.att.output.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.17.ffn.time_mix_k, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.17.ffn.time_mix_r, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.17.ffn.key.weight, shape torch.Size([10240, 2560]), type torch.float16\n",
            "Writing blocks.17.ffn.receptance.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.17.ffn.value.weight, shape torch.Size([2560, 10240]), type torch.float16\n",
            "Writing blocks.18.ln1.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.18.ln1.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.18.ln2.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.18.ln2.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.18.att.time_decay, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.18.att.time_first, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.18.att.time_mix_k, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.18.att.time_mix_v, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.18.att.time_mix_r, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.18.att.key.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.18.att.value.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.18.att.receptance.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.18.att.output.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.18.ffn.time_mix_k, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.18.ffn.time_mix_r, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.18.ffn.key.weight, shape torch.Size([10240, 2560]), type torch.float16\n",
            "Writing blocks.18.ffn.receptance.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.18.ffn.value.weight, shape torch.Size([2560, 10240]), type torch.float16\n",
            "Writing blocks.19.ln1.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.19.ln1.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.19.ln2.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.19.ln2.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.19.att.time_decay, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.19.att.time_first, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.19.att.time_mix_k, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.19.att.time_mix_v, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.19.att.time_mix_r, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.19.att.key.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.19.att.value.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.19.att.receptance.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.19.att.output.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.19.ffn.time_mix_k, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.19.ffn.time_mix_r, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.19.ffn.key.weight, shape torch.Size([10240, 2560]), type torch.float16\n",
            "Writing blocks.19.ffn.receptance.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.19.ffn.value.weight, shape torch.Size([2560, 10240]), type torch.float16\n",
            "Writing blocks.20.ln1.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.20.ln1.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.20.ln2.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.20.ln2.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.20.att.time_decay, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.20.att.time_first, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.20.att.time_mix_k, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.20.att.time_mix_v, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.20.att.time_mix_r, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.20.att.key.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.20.att.value.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.20.att.receptance.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.20.att.output.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.20.ffn.time_mix_k, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.20.ffn.time_mix_r, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.20.ffn.key.weight, shape torch.Size([10240, 2560]), type torch.float16\n",
            "Writing blocks.20.ffn.receptance.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.20.ffn.value.weight, shape torch.Size([2560, 10240]), type torch.float16\n",
            "Writing blocks.21.ln1.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.21.ln1.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.21.ln2.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.21.ln2.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.21.att.time_decay, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.21.att.time_first, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.21.att.time_mix_k, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.21.att.time_mix_v, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.21.att.time_mix_r, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.21.att.key.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.21.att.value.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.21.att.receptance.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.21.att.output.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.21.ffn.time_mix_k, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.21.ffn.time_mix_r, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.21.ffn.key.weight, shape torch.Size([10240, 2560]), type torch.float16\n",
            "Writing blocks.21.ffn.receptance.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.21.ffn.value.weight, shape torch.Size([2560, 10240]), type torch.float16\n",
            "Writing blocks.22.ln1.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.22.ln1.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.22.ln2.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.22.ln2.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.22.att.time_decay, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.22.att.time_first, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.22.att.time_mix_k, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.22.att.time_mix_v, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.22.att.time_mix_r, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.22.att.key.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.22.att.value.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.22.att.receptance.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.22.att.output.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.22.ffn.time_mix_k, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.22.ffn.time_mix_r, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.22.ffn.key.weight, shape torch.Size([10240, 2560]), type torch.float16\n",
            "Writing blocks.22.ffn.receptance.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.22.ffn.value.weight, shape torch.Size([2560, 10240]), type torch.float16\n",
            "Writing blocks.23.ln1.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.23.ln1.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.23.ln2.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.23.ln2.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.23.att.time_decay, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.23.att.time_first, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.23.att.time_mix_k, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.23.att.time_mix_v, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.23.att.time_mix_r, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.23.att.key.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.23.att.value.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.23.att.receptance.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.23.att.output.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.23.ffn.time_mix_k, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.23.ffn.time_mix_r, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.23.ffn.key.weight, shape torch.Size([10240, 2560]), type torch.float16\n",
            "Writing blocks.23.ffn.receptance.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.23.ffn.value.weight, shape torch.Size([2560, 10240]), type torch.float16\n",
            "Writing blocks.24.ln1.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.24.ln1.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.24.ln2.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.24.ln2.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.24.att.time_decay, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.24.att.time_first, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.24.att.time_mix_k, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.24.att.time_mix_v, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.24.att.time_mix_r, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.24.att.key.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.24.att.value.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.24.att.receptance.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.24.att.output.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.24.ffn.time_mix_k, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.24.ffn.time_mix_r, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.24.ffn.key.weight, shape torch.Size([10240, 2560]), type torch.float16\n",
            "Writing blocks.24.ffn.receptance.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.24.ffn.value.weight, shape torch.Size([2560, 10240]), type torch.float16\n",
            "Writing blocks.25.ln1.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.25.ln1.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.25.ln2.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.25.ln2.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.25.att.time_decay, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.25.att.time_first, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.25.att.time_mix_k, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.25.att.time_mix_v, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.25.att.time_mix_r, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.25.att.key.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.25.att.value.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.25.att.receptance.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.25.att.output.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.25.ffn.time_mix_k, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.25.ffn.time_mix_r, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.25.ffn.key.weight, shape torch.Size([10240, 2560]), type torch.float16\n",
            "Writing blocks.25.ffn.receptance.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.25.ffn.value.weight, shape torch.Size([2560, 10240]), type torch.float16\n",
            "Writing blocks.26.ln1.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.26.ln1.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.26.ln2.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.26.ln2.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.26.att.time_decay, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.26.att.time_first, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.26.att.time_mix_k, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.26.att.time_mix_v, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.26.att.time_mix_r, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.26.att.key.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.26.att.value.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.26.att.receptance.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.26.att.output.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.26.ffn.time_mix_k, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.26.ffn.time_mix_r, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.26.ffn.key.weight, shape torch.Size([10240, 2560]), type torch.float16\n",
            "Writing blocks.26.ffn.receptance.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.26.ffn.value.weight, shape torch.Size([2560, 10240]), type torch.float16\n",
            "Writing blocks.27.ln1.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.27.ln1.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.27.ln2.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.27.ln2.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.27.att.time_decay, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.27.att.time_first, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.27.att.time_mix_k, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.27.att.time_mix_v, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.27.att.time_mix_r, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.27.att.key.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.27.att.value.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.27.att.receptance.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.27.att.output.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.27.ffn.time_mix_k, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.27.ffn.time_mix_r, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.27.ffn.key.weight, shape torch.Size([10240, 2560]), type torch.float16\n",
            "Writing blocks.27.ffn.receptance.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.27.ffn.value.weight, shape torch.Size([2560, 10240]), type torch.float16\n",
            "Writing blocks.28.ln1.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.28.ln1.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.28.ln2.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.28.ln2.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.28.att.time_decay, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.28.att.time_first, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.28.att.time_mix_k, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.28.att.time_mix_v, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.28.att.time_mix_r, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.28.att.key.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.28.att.value.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.28.att.receptance.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.28.att.output.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.28.ffn.time_mix_k, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.28.ffn.time_mix_r, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.28.ffn.key.weight, shape torch.Size([10240, 2560]), type torch.float16\n",
            "Writing blocks.28.ffn.receptance.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.28.ffn.value.weight, shape torch.Size([2560, 10240]), type torch.float16\n",
            "Writing blocks.29.ln1.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.29.ln1.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.29.ln2.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.29.ln2.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.29.att.time_decay, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.29.att.time_first, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.29.att.time_mix_k, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.29.att.time_mix_v, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.29.att.time_mix_r, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.29.att.key.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.29.att.value.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.29.att.receptance.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.29.att.output.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.29.ffn.time_mix_k, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.29.ffn.time_mix_r, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.29.ffn.key.weight, shape torch.Size([10240, 2560]), type torch.float16\n",
            "Writing blocks.29.ffn.receptance.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.29.ffn.value.weight, shape torch.Size([2560, 10240]), type torch.float16\n",
            "Writing blocks.30.ln1.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.30.ln1.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.30.ln2.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.30.ln2.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.30.att.time_decay, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.30.att.time_first, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.30.att.time_mix_k, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.30.att.time_mix_v, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.30.att.time_mix_r, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.30.att.key.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.30.att.value.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.30.att.receptance.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.30.att.output.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.30.ffn.time_mix_k, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.30.ffn.time_mix_r, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.30.ffn.key.weight, shape torch.Size([10240, 2560]), type torch.float16\n",
            "Writing blocks.30.ffn.receptance.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.30.ffn.value.weight, shape torch.Size([2560, 10240]), type torch.float16\n",
            "Writing blocks.31.ln1.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.31.ln1.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.31.ln2.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.31.ln2.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.31.att.time_decay, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.31.att.time_first, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.31.att.time_mix_k, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.31.att.time_mix_v, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.31.att.time_mix_r, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.31.att.key.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.31.att.value.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.31.att.receptance.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.31.att.output.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.31.ffn.time_mix_k, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.31.ffn.time_mix_r, shape torch.Size([2560]), type torch.float32\n",
            "Writing blocks.31.ffn.key.weight, shape torch.Size([10240, 2560]), type torch.float16\n",
            "Writing blocks.31.ffn.receptance.weight, shape torch.Size([2560, 2560]), type torch.float16\n",
            "Writing blocks.31.ffn.value.weight, shape torch.Size([2560, 10240]), type torch.float16\n",
            "Writing ln_out.weight, shape torch.Size([2560]), type torch.float32\n",
            "Writing ln_out.bias, shape torch.Size([2560]), type torch.float32\n",
            "Writing head.weight, shape torch.Size([65536, 2560]), type torch.float16\n",
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quantizing"
      ],
      "metadata": {
        "id": "Xzipi-xo2Iom"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python rwkv.cpp/python/quantize.py RWKV-4-World-3B-v1-20230619-ctx4096.bin RWKV-4-World-3B-v1-20230619-ctx4096_Q5_1.bin Q5_1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxxUhzF_1dyW",
        "outputId": "3b319172-b8ec-4d91-c83a-b4da16ab4bab"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from 'RWKV-4-World-3B-v1-20230619-ctx4096.bin'\n",
            "                     emb.weight - [ 2560, 65536], type =   FP16 size =  320.000 MB\n",
            "            blocks.0.ln1.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "              blocks.0.ln1.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "            blocks.0.ln2.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "              blocks.0.ln2.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "            blocks.0.ln0.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "              blocks.0.ln0.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.0.att.time_decay - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.0.att.time_first - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.0.att.time_mix_k - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.0.att.time_mix_v - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.0.att.time_mix_r - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.0.att.key.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.064 0.065 0.078 0.070 0.058 0.058 0.056 0.055 0.054 0.054 0.066 \n",
            "      blocks.0.att.value.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.076 0.061 0.061 0.061 0.063 0.064 0.066 0.080 0.071 0.058 0.057 0.055 0.054 0.054 0.054 0.066 \n",
            " blocks.0.att.receptance.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.053 0.065 \n",
            "     blocks.0.att.output.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.064 0.066 0.079 0.070 0.058 0.057 0.056 0.054 0.054 0.054 0.066 \n",
            "        blocks.0.ffn.time_mix_k - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.0.ffn.time_mix_r - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.0.ffn.key.weight - [ 2560, 10240], type =   FP16 quantizing... size =    50.00 MB ->    18.75 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.079 0.070 0.058 0.057 0.056 0.055 0.054 0.053 0.066 \n",
            " blocks.0.ffn.receptance.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.074 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.053 0.066 \n",
            "      blocks.0.ffn.value.weight - [10240,  2560], type =   FP16 quantizing... size =    50.00 MB ->    18.75 MB | hist: 0.074 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
            "            blocks.1.ln1.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "              blocks.1.ln1.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "            blocks.1.ln2.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "              blocks.1.ln2.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.1.att.time_decay - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.1.att.time_first - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.1.att.time_mix_k - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.1.att.time_mix_v - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.1.att.time_mix_r - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.1.att.key.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
            "      blocks.1.att.value.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.079 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            " blocks.1.att.receptance.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.074 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.058 0.056 0.055 0.054 0.054 0.066 \n",
            "     blocks.1.att.output.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.061 0.061 0.063 0.065 0.066 0.079 0.070 0.058 0.057 0.056 0.054 0.054 0.053 0.066 \n",
            "        blocks.1.ffn.time_mix_k - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.1.ffn.time_mix_r - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.1.ffn.key.weight - [ 2560, 10240], type =   FP16 quantizing... size =    50.00 MB ->    18.75 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            " blocks.1.ffn.receptance.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.074 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "      blocks.1.ffn.value.weight - [10240,  2560], type =   FP16 quantizing... size =    50.00 MB ->    18.75 MB | hist: 0.074 0.061 0.062 0.063 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
            "            blocks.2.ln1.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "              blocks.2.ln1.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "            blocks.2.ln2.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "              blocks.2.ln2.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.2.att.time_decay - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.2.att.time_first - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.2.att.time_mix_k - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.2.att.time_mix_v - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.2.att.time_mix_r - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.2.att.key.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.074 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.058 0.056 0.055 0.054 0.054 0.066 \n",
            "      blocks.2.att.value.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.079 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            " blocks.2.att.receptance.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.064 0.065 0.078 0.070 0.058 0.058 0.056 0.055 0.054 0.054 0.066 \n",
            "     blocks.2.att.output.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.061 0.061 0.063 0.065 0.066 0.079 0.070 0.058 0.057 0.056 0.054 0.054 0.053 0.066 \n",
            "        blocks.2.ffn.time_mix_k - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.2.ffn.time_mix_r - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.2.ffn.key.weight - [ 2560, 10240], type =   FP16 quantizing... size =    50.00 MB ->    18.75 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            " blocks.2.ffn.receptance.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.062 0.063 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
            "      blocks.2.ffn.value.weight - [10240,  2560], type =   FP16 quantizing... size =    50.00 MB ->    18.75 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
            "            blocks.3.ln1.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "              blocks.3.ln1.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "            blocks.3.ln2.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "              blocks.3.ln2.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.3.att.time_decay - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.3.att.time_first - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.3.att.time_mix_k - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.3.att.time_mix_v - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.3.att.time_mix_r - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.3.att.key.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.079 0.070 0.058 0.057 0.056 0.055 0.054 0.053 0.065 \n",
            "      blocks.3.att.value.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.066 0.079 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            " blocks.3.att.receptance.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.074 0.061 0.061 0.062 0.063 0.064 0.065 0.078 0.070 0.058 0.058 0.056 0.055 0.054 0.054 0.066 \n",
            "     blocks.3.att.output.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.061 0.061 0.063 0.065 0.066 0.079 0.070 0.058 0.057 0.056 0.054 0.053 0.054 0.066 \n",
            "        blocks.3.ffn.time_mix_k - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.3.ffn.time_mix_r - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.3.ffn.key.weight - [ 2560, 10240], type =   FP16 quantizing... size =    50.00 MB ->    18.75 MB | hist: 0.075 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            " blocks.3.ffn.receptance.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.058 0.056 0.055 0.054 0.054 0.065 \n",
            "      blocks.3.ffn.value.weight - [10240,  2560], type =   FP16 quantizing... size =    50.00 MB ->    18.75 MB | hist: 0.075 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
            "            blocks.4.ln1.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "              blocks.4.ln1.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "            blocks.4.ln2.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "              blocks.4.ln2.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.4.att.time_decay - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.4.att.time_first - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.4.att.time_mix_k - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.4.att.time_mix_v - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.4.att.time_mix_r - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.4.att.key.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.062 0.063 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.053 0.065 \n",
            "      blocks.4.att.value.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.066 0.079 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            " blocks.4.att.receptance.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.074 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.058 0.056 0.055 0.054 0.054 0.066 \n",
            "     blocks.4.att.output.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.061 0.061 0.063 0.065 0.066 0.079 0.070 0.058 0.057 0.056 0.054 0.054 0.053 0.066 \n",
            "        blocks.4.ffn.time_mix_k - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.4.ffn.time_mix_r - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.4.ffn.key.weight - [ 2560, 10240], type =   FP16 quantizing... size =    50.00 MB ->    18.75 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
            " blocks.4.ffn.receptance.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
            "      blocks.4.ffn.value.weight - [10240,  2560], type =   FP16 quantizing... size =    50.00 MB ->    18.75 MB | hist: 0.074 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
            "            blocks.5.ln1.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "              blocks.5.ln1.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "            blocks.5.ln2.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "              blocks.5.ln2.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.5.att.time_decay - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.5.att.time_first - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.5.att.time_mix_k - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.5.att.time_mix_v - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.5.att.time_mix_r - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.5.att.key.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.074 0.061 0.062 0.063 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
            "      blocks.5.att.value.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.079 0.070 0.058 0.057 0.056 0.055 0.054 0.053 0.066 \n",
            " blocks.5.att.receptance.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.058 0.056 0.055 0.054 0.054 0.066 \n",
            "     blocks.5.att.output.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.061 0.061 0.063 0.065 0.066 0.079 0.070 0.058 0.057 0.056 0.054 0.054 0.054 0.066 \n",
            "        blocks.5.ffn.time_mix_k - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.5.ffn.time_mix_r - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.5.ffn.key.weight - [ 2560, 10240], type =   FP16 quantizing... size =    50.00 MB ->    18.75 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            " blocks.5.ffn.receptance.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.062 0.063 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.053 0.065 \n",
            "      blocks.5.ffn.value.weight - [10240,  2560], type =   FP16 quantizing... size =    50.00 MB ->    18.75 MB | hist: 0.075 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
            "            blocks.6.ln1.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "              blocks.6.ln1.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "            blocks.6.ln2.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "              blocks.6.ln2.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.6.att.time_decay - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.6.att.time_first - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.6.att.time_mix_k - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.6.att.time_mix_v - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.6.att.time_mix_r - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.6.att.key.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.074 0.061 0.061 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.058 0.056 0.055 0.054 0.054 0.065 \n",
            "      blocks.6.att.value.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.079 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            " blocks.6.att.receptance.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.074 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "     blocks.6.att.output.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.066 0.079 0.070 0.058 0.057 0.056 0.054 0.054 0.054 0.066 \n",
            "        blocks.6.ffn.time_mix_k - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.6.ffn.time_mix_r - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.6.ffn.key.weight - [ 2560, 10240], type =   FP16 quantizing... size =    50.00 MB ->    18.75 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            " blocks.6.ffn.receptance.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.057 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "      blocks.6.ffn.value.weight - [10240,  2560], type =   FP16 quantizing... size =    50.00 MB ->    18.75 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "            blocks.7.ln1.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "              blocks.7.ln1.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "            blocks.7.ln2.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "              blocks.7.ln2.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.7.att.time_decay - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.7.att.time_first - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.7.att.time_mix_k - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.7.att.time_mix_v - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.7.att.time_mix_r - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.7.att.key.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.074 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.058 0.057 0.055 0.054 0.054 0.066 \n",
            "      blocks.7.att.value.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.064 0.065 0.079 0.070 0.058 0.058 0.056 0.055 0.054 0.054 0.066 \n",
            " blocks.7.att.receptance.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.074 0.061 0.061 0.062 0.063 0.064 0.065 0.078 0.070 0.058 0.058 0.056 0.055 0.054 0.054 0.066 \n",
            "     blocks.7.att.output.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.066 0.079 0.070 0.058 0.057 0.056 0.054 0.054 0.054 0.066 \n",
            "        blocks.7.ffn.time_mix_k - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.7.ffn.time_mix_r - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.7.ffn.key.weight - [ 2560, 10240], type =   FP16 quantizing... size =    50.00 MB ->    18.75 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            " blocks.7.ffn.receptance.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.057 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "      blocks.7.ffn.value.weight - [10240,  2560], type =   FP16 quantizing... size =    50.00 MB ->    18.75 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "            blocks.8.ln1.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "              blocks.8.ln1.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "            blocks.8.ln2.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "              blocks.8.ln2.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.8.att.time_decay - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.8.att.time_first - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.8.att.time_mix_k - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.8.att.time_mix_v - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.8.att.time_mix_r - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.8.att.key.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.074 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.058 0.057 0.055 0.054 0.053 0.065 \n",
            "      blocks.8.att.value.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.066 0.079 0.070 0.058 0.057 0.056 0.054 0.054 0.054 0.066 \n",
            " blocks.8.att.receptance.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.074 0.061 0.061 0.062 0.063 0.064 0.065 0.078 0.070 0.058 0.058 0.056 0.055 0.054 0.054 0.066 \n",
            "     blocks.8.att.output.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.061 0.061 0.063 0.065 0.066 0.079 0.070 0.058 0.057 0.056 0.054 0.054 0.054 0.066 \n",
            "        blocks.8.ffn.time_mix_k - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.8.ffn.time_mix_r - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.8.ffn.key.weight - [ 2560, 10240], type =   FP16 quantizing... size =    50.00 MB ->    18.75 MB | hist: 0.075 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            " blocks.8.ffn.receptance.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.062 0.063 0.064 0.064 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.055 0.054 0.065 \n",
            "      blocks.8.ffn.value.weight - [10240,  2560], type =   FP16 quantizing... size =    50.00 MB ->    18.75 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "            blocks.9.ln1.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "              blocks.9.ln1.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "            blocks.9.ln2.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "              blocks.9.ln2.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.9.att.time_decay - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.9.att.time_first - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.9.att.time_mix_k - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.9.att.time_mix_v - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.9.att.time_mix_r - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.9.att.key.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.074 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.058 0.056 0.055 0.054 0.053 0.065 \n",
            "      blocks.9.att.value.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.066 0.079 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            " blocks.9.att.receptance.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "     blocks.9.att.output.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.061 0.061 0.063 0.065 0.066 0.079 0.070 0.058 0.057 0.056 0.054 0.054 0.054 0.066 \n",
            "        blocks.9.ffn.time_mix_k - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.9.ffn.time_mix_r - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "        blocks.9.ffn.key.weight - [ 2560, 10240], type =   FP16 quantizing... size =    50.00 MB ->    18.75 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            " blocks.9.ffn.receptance.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.062 0.063 0.064 0.065 0.065 0.078 0.069 0.057 0.057 0.056 0.055 0.054 0.054 0.065 \n",
            "      blocks.9.ffn.value.weight - [10240,  2560], type =   FP16 quantizing... size =    50.00 MB ->    18.75 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.079 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "           blocks.10.ln1.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "             blocks.10.ln1.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "           blocks.10.ln2.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "             blocks.10.ln2.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.10.att.time_decay - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.10.att.time_first - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.10.att.time_mix_k - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.10.att.time_mix_v - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.10.att.time_mix_r - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.10.att.key.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.074 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.058 0.056 0.055 0.054 0.053 0.065 \n",
            "     blocks.10.att.value.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.079 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "blocks.10.att.receptance.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "    blocks.10.att.output.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.061 0.061 0.063 0.065 0.066 0.079 0.070 0.058 0.057 0.056 0.054 0.054 0.053 0.066 \n",
            "       blocks.10.ffn.time_mix_k - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.10.ffn.time_mix_r - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.10.ffn.key.weight - [ 2560, 10240], type =   FP16 quantizing... size =    50.00 MB ->    18.75 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "blocks.10.ffn.receptance.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.062 0.063 0.064 0.065 0.065 0.078 0.069 0.057 0.057 0.056 0.055 0.055 0.054 0.065 \n",
            "     blocks.10.ffn.value.weight - [10240,  2560], type =   FP16 quantizing... size =    50.00 MB ->    18.75 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "           blocks.11.ln1.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "             blocks.11.ln1.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "           blocks.11.ln2.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "             blocks.11.ln2.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.11.att.time_decay - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.11.att.time_first - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.11.att.time_mix_k - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.11.att.time_mix_v - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.11.att.time_mix_r - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.11.att.key.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.074 0.061 0.061 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.058 0.057 0.055 0.054 0.053 0.065 \n",
            "     blocks.11.att.value.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.066 0.079 0.070 0.058 0.057 0.056 0.054 0.054 0.054 0.066 \n",
            "blocks.11.att.receptance.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "    blocks.11.att.output.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.061 0.061 0.063 0.065 0.066 0.079 0.070 0.058 0.057 0.056 0.054 0.054 0.054 0.066 \n",
            "       blocks.11.ffn.time_mix_k - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.11.ffn.time_mix_r - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.11.ffn.key.weight - [ 2560, 10240], type =   FP16 quantizing... size =    50.00 MB ->    18.75 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "blocks.11.ffn.receptance.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.062 0.063 0.064 0.065 0.065 0.078 0.069 0.057 0.057 0.056 0.055 0.054 0.054 0.065 \n",
            "     blocks.11.ffn.value.weight - [10240,  2560], type =   FP16 quantizing... size =    50.00 MB ->    18.75 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "           blocks.12.ln1.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "             blocks.12.ln1.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "           blocks.12.ln2.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "             blocks.12.ln2.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.12.att.time_decay - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.12.att.time_first - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.12.att.time_mix_k - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.12.att.time_mix_v - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.12.att.time_mix_r - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.12.att.key.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.074 0.061 0.061 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.058 0.056 0.055 0.054 0.054 0.065 \n",
            "     blocks.12.att.value.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.066 0.079 0.070 0.058 0.057 0.056 0.054 0.054 0.054 0.066 \n",
            "blocks.12.att.receptance.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.058 0.056 0.055 0.054 0.054 0.066 \n",
            "    blocks.12.att.output.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.061 0.061 0.063 0.065 0.066 0.079 0.071 0.059 0.058 0.055 0.054 0.054 0.053 0.066 \n",
            "       blocks.12.ffn.time_mix_k - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.12.ffn.time_mix_r - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.12.ffn.key.weight - [ 2560, 10240], type =   FP16 quantizing... size =    50.00 MB ->    18.75 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "blocks.12.ffn.receptance.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.062 0.062 0.063 0.064 0.065 0.065 0.078 0.069 0.057 0.057 0.056 0.055 0.055 0.054 0.065 \n",
            "     blocks.12.ffn.value.weight - [10240,  2560], type =   FP16 quantizing... size =    50.00 MB ->    18.75 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "           blocks.13.ln1.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "             blocks.13.ln1.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "           blocks.13.ln2.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "             blocks.13.ln2.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.13.att.time_decay - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.13.att.time_first - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.13.att.time_mix_k - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.13.att.time_mix_v - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.13.att.time_mix_r - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.13.att.key.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.074 0.061 0.061 0.062 0.064 0.065 0.065 0.078 0.070 0.058 0.058 0.057 0.055 0.054 0.053 0.065 \n",
            "     blocks.13.att.value.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.066 0.079 0.070 0.058 0.057 0.056 0.054 0.054 0.054 0.066 \n",
            "blocks.13.att.receptance.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.064 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "    blocks.13.att.output.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.076 0.061 0.061 0.061 0.063 0.065 0.066 0.079 0.071 0.059 0.057 0.055 0.054 0.053 0.053 0.066 \n",
            "       blocks.13.ffn.time_mix_k - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.13.ffn.time_mix_r - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.13.ffn.key.weight - [ 2560, 10240], type =   FP16 quantizing... size =    50.00 MB ->    18.75 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "blocks.13.ffn.receptance.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.062 0.063 0.064 0.065 0.065 0.078 0.069 0.057 0.057 0.056 0.055 0.055 0.054 0.065 \n",
            "     blocks.13.ffn.value.weight - [10240,  2560], type =   FP16 quantizing... size =    50.00 MB ->    18.75 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.079 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "           blocks.14.ln1.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "             blocks.14.ln1.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "           blocks.14.ln2.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "             blocks.14.ln2.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.14.att.time_decay - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.14.att.time_first - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.14.att.time_mix_k - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.14.att.time_mix_v - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.14.att.time_mix_r - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.14.att.key.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.074 0.061 0.061 0.062 0.063 0.064 0.065 0.078 0.070 0.058 0.058 0.057 0.055 0.054 0.054 0.066 \n",
            "     blocks.14.att.value.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.066 0.079 0.070 0.058 0.058 0.056 0.054 0.054 0.053 0.066 \n",
            "blocks.14.att.receptance.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.053 0.066 \n",
            "    blocks.14.att.output.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.076 0.061 0.060 0.060 0.062 0.065 0.066 0.080 0.072 0.059 0.058 0.055 0.053 0.053 0.053 0.067 \n",
            "       blocks.14.ffn.time_mix_k - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.14.ffn.time_mix_r - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.14.ffn.key.weight - [ 2560, 10240], type =   FP16 quantizing... size =    50.00 MB ->    18.75 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "blocks.14.ffn.receptance.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.062 0.063 0.064 0.065 0.065 0.078 0.069 0.057 0.057 0.056 0.055 0.054 0.054 0.065 \n",
            "     blocks.14.ffn.value.weight - [10240,  2560], type =   FP16 quantizing... size =    50.00 MB ->    18.75 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.079 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "           blocks.15.ln1.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "             blocks.15.ln1.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "           blocks.15.ln2.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "             blocks.15.ln2.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.15.att.time_decay - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.15.att.time_first - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.15.att.time_mix_k - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.15.att.time_mix_v - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.15.att.time_mix_r - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.15.att.key.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.060 0.061 0.061 0.063 0.065 0.066 0.079 0.071 0.059 0.058 0.056 0.054 0.054 0.053 0.066 \n",
            "     blocks.15.att.value.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.061 0.061 0.063 0.065 0.066 0.080 0.071 0.059 0.057 0.056 0.054 0.053 0.053 0.066 \n",
            "blocks.15.att.receptance.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.066 0.079 0.070 0.058 0.057 0.055 0.054 0.054 0.053 0.065 \n",
            "    blocks.15.att.output.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.076 0.061 0.060 0.060 0.062 0.065 0.067 0.080 0.071 0.059 0.058 0.055 0.053 0.053 0.053 0.067 \n",
            "       blocks.15.ffn.time_mix_k - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.15.ffn.time_mix_r - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.15.ffn.key.weight - [ 2560, 10240], type =   FP16 quantizing... size =    50.00 MB ->    18.75 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "blocks.15.ffn.receptance.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.074 0.061 0.062 0.063 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.053 0.065 \n",
            "     blocks.15.ffn.value.weight - [10240,  2560], type =   FP16 quantizing... size =    50.00 MB ->    18.75 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.079 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "           blocks.16.ln1.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "             blocks.16.ln1.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "           blocks.16.ln2.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "             blocks.16.ln2.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.16.att.time_decay - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.16.att.time_first - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.16.att.time_mix_k - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.16.att.time_mix_v - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.16.att.time_mix_r - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.16.att.key.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.060 0.061 0.061 0.063 0.065 0.066 0.079 0.071 0.059 0.058 0.056 0.054 0.054 0.053 0.066 \n",
            "     blocks.16.att.value.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.060 0.061 0.062 0.065 0.066 0.080 0.071 0.059 0.058 0.056 0.054 0.053 0.053 0.066 \n",
            "blocks.16.att.receptance.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.066 0.079 0.070 0.058 0.057 0.055 0.054 0.053 0.053 0.066 \n",
            "    blocks.16.att.output.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.076 0.061 0.060 0.061 0.062 0.065 0.067 0.080 0.071 0.059 0.057 0.055 0.053 0.053 0.053 0.066 \n",
            "       blocks.16.ffn.time_mix_k - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.16.ffn.time_mix_r - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.16.ffn.key.weight - [ 2560, 10240], type =   FP16 quantizing... size =    50.00 MB ->    18.75 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "blocks.16.ffn.receptance.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.062 0.063 0.064 0.065 0.065 0.078 0.069 0.057 0.057 0.056 0.055 0.054 0.053 0.065 \n",
            "     blocks.16.ffn.value.weight - [10240,  2560], type =   FP16 quantizing... size =    50.00 MB ->    18.75 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "           blocks.17.ln1.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "             blocks.17.ln1.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "           blocks.17.ln2.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "             blocks.17.ln2.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.17.att.time_decay - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.17.att.time_first - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.17.att.time_mix_k - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.17.att.time_mix_v - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.17.att.time_mix_r - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.17.att.key.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.060 0.060 0.061 0.063 0.065 0.066 0.079 0.071 0.059 0.058 0.056 0.054 0.054 0.053 0.066 \n",
            "     blocks.17.att.value.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.076 0.061 0.060 0.061 0.063 0.065 0.066 0.080 0.071 0.059 0.058 0.055 0.054 0.053 0.053 0.066 \n",
            "blocks.17.att.receptance.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.066 0.079 0.069 0.058 0.057 0.056 0.054 0.054 0.053 0.065 \n",
            "    blocks.17.att.output.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.076 0.061 0.060 0.061 0.063 0.065 0.066 0.080 0.071 0.059 0.058 0.055 0.054 0.053 0.053 0.066 \n",
            "       blocks.17.ffn.time_mix_k - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.17.ffn.time_mix_r - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.17.ffn.key.weight - [ 2560, 10240], type =   FP16 quantizing... size =    50.00 MB ->    18.75 MB | hist: 0.075 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
            "blocks.17.ffn.receptance.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.062 0.063 0.064 0.065 0.065 0.078 0.069 0.057 0.057 0.056 0.055 0.054 0.054 0.065 \n",
            "     blocks.17.ffn.value.weight - [10240,  2560], type =   FP16 quantizing... size =    50.00 MB ->    18.75 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "           blocks.18.ln1.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "             blocks.18.ln1.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "           blocks.18.ln2.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "             blocks.18.ln2.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.18.att.time_decay - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.18.att.time_first - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.18.att.time_mix_k - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.18.att.time_mix_v - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.18.att.time_mix_r - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.18.att.key.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.060 0.061 0.061 0.062 0.064 0.066 0.079 0.071 0.059 0.058 0.056 0.055 0.054 0.053 0.066 \n",
            "     blocks.18.att.value.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.076 0.061 0.060 0.061 0.062 0.065 0.066 0.080 0.071 0.059 0.057 0.055 0.054 0.053 0.053 0.066 \n",
            "blocks.18.att.receptance.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.076 0.062 0.062 0.062 0.064 0.065 0.066 0.079 0.069 0.058 0.057 0.056 0.054 0.053 0.053 0.065 \n",
            "    blocks.18.att.output.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.076 0.061 0.061 0.061 0.063 0.065 0.066 0.080 0.071 0.059 0.057 0.055 0.054 0.053 0.053 0.066 \n",
            "       blocks.18.ffn.time_mix_k - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.18.ffn.time_mix_r - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.18.ffn.key.weight - [ 2560, 10240], type =   FP16 quantizing... size =    50.00 MB ->    18.75 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "blocks.18.ffn.receptance.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.062 0.063 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.053 0.065 \n",
            "     blocks.18.ffn.value.weight - [10240,  2560], type =   FP16 quantizing... size =    50.00 MB ->    18.75 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "           blocks.19.ln1.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "             blocks.19.ln1.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "           blocks.19.ln2.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "             blocks.19.ln2.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.19.att.time_decay - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.19.att.time_first - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.19.att.time_mix_k - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.19.att.time_mix_v - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.19.att.time_mix_r - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.19.att.key.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.074 0.060 0.061 0.062 0.063 0.065 0.065 0.078 0.071 0.059 0.058 0.057 0.055 0.054 0.053 0.066 \n",
            "     blocks.19.att.value.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.060 0.061 0.063 0.065 0.066 0.080 0.071 0.059 0.058 0.055 0.054 0.053 0.053 0.066 \n",
            "blocks.19.att.receptance.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.066 0.079 0.070 0.058 0.057 0.056 0.054 0.054 0.053 0.065 \n",
            "    blocks.19.att.output.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.076 0.061 0.060 0.061 0.063 0.065 0.066 0.080 0.071 0.059 0.058 0.055 0.054 0.053 0.053 0.066 \n",
            "       blocks.19.ffn.time_mix_k - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.19.ffn.time_mix_r - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.19.ffn.key.weight - [ 2560, 10240], type =   FP16 quantizing... size =    50.00 MB ->    18.75 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "blocks.19.ffn.receptance.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
            "     blocks.19.ffn.value.weight - [10240,  2560], type =   FP16 quantizing... size =    50.00 MB ->    18.75 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "           blocks.20.ln1.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "             blocks.20.ln1.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "           blocks.20.ln2.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "             blocks.20.ln2.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.20.att.time_decay - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.20.att.time_first - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.20.att.time_mix_k - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.20.att.time_mix_v - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.20.att.time_mix_r - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.20.att.key.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.074 0.060 0.061 0.061 0.063 0.065 0.065 0.078 0.070 0.059 0.058 0.057 0.055 0.054 0.053 0.066 \n",
            "     blocks.20.att.value.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.060 0.061 0.063 0.065 0.066 0.080 0.071 0.059 0.058 0.055 0.054 0.053 0.053 0.066 \n",
            "blocks.20.att.receptance.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.066 0.079 0.070 0.058 0.057 0.056 0.054 0.054 0.053 0.066 \n",
            "    blocks.20.att.output.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.060 0.061 0.063 0.065 0.066 0.080 0.071 0.059 0.058 0.055 0.054 0.053 0.053 0.066 \n",
            "       blocks.20.ffn.time_mix_k - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.20.ffn.time_mix_r - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.20.ffn.key.weight - [ 2560, 10240], type =   FP16 quantizing... size =    50.00 MB ->    18.75 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "blocks.20.ffn.receptance.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.057 0.057 0.056 0.055 0.054 0.053 0.065 \n",
            "     blocks.20.ffn.value.weight - [10240,  2560], type =   FP16 quantizing... size =    50.00 MB ->    18.75 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "           blocks.21.ln1.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "             blocks.21.ln1.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "           blocks.21.ln2.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "             blocks.21.ln2.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.21.att.time_decay - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.21.att.time_first - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.21.att.time_mix_k - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.21.att.time_mix_v - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.21.att.time_mix_r - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.21.att.key.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.074 0.060 0.061 0.061 0.063 0.065 0.065 0.078 0.070 0.059 0.058 0.057 0.055 0.054 0.054 0.066 \n",
            "     blocks.21.att.value.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.061 0.061 0.063 0.065 0.066 0.080 0.071 0.058 0.057 0.055 0.054 0.053 0.053 0.066 \n",
            "blocks.21.att.receptance.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.066 0.078 0.070 0.058 0.057 0.056 0.054 0.054 0.053 0.066 \n",
            "    blocks.21.att.output.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.060 0.061 0.063 0.065 0.066 0.080 0.071 0.059 0.058 0.055 0.054 0.053 0.053 0.066 \n",
            "       blocks.21.ffn.time_mix_k - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.21.ffn.time_mix_r - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.21.ffn.key.weight - [ 2560, 10240], type =   FP16 quantizing... size =    50.00 MB ->    18.75 MB | hist: 0.074 0.061 0.062 0.062 0.063 0.064 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "blocks.21.ffn.receptance.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.062 0.063 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.053 0.065 \n",
            "     blocks.21.ffn.value.weight - [10240,  2560], type =   FP16 quantizing... size =    50.00 MB ->    18.75 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "           blocks.22.ln1.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "             blocks.22.ln1.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "           blocks.22.ln2.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "             blocks.22.ln2.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.22.att.time_decay - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.22.att.time_first - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.22.att.time_mix_k - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.22.att.time_mix_v - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.22.att.time_mix_r - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.22.att.key.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.074 0.060 0.061 0.062 0.063 0.064 0.065 0.078 0.070 0.059 0.058 0.057 0.056 0.054 0.054 0.066 \n",
            "     blocks.22.att.value.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.061 0.061 0.063 0.065 0.066 0.079 0.070 0.058 0.058 0.056 0.054 0.053 0.053 0.066 \n",
            "blocks.22.att.receptance.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "    blocks.22.att.output.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.076 0.061 0.060 0.061 0.062 0.065 0.066 0.080 0.071 0.059 0.057 0.055 0.054 0.053 0.054 0.067 \n",
            "       blocks.22.ffn.time_mix_k - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.22.ffn.time_mix_r - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.22.ffn.key.weight - [ 2560, 10240], type =   FP16 quantizing... size =    50.00 MB ->    18.75 MB | hist: 0.074 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "blocks.22.ffn.receptance.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
            "     blocks.22.ffn.value.weight - [10240,  2560], type =   FP16 quantizing... size =    50.00 MB ->    18.75 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "           blocks.23.ln1.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "             blocks.23.ln1.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "           blocks.23.ln2.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "             blocks.23.ln2.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.23.att.time_decay - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.23.att.time_first - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.23.att.time_mix_k - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.23.att.time_mix_v - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.23.att.time_mix_r - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.23.att.key.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.074 0.060 0.061 0.062 0.063 0.064 0.065 0.078 0.070 0.058 0.058 0.057 0.055 0.054 0.054 0.066 \n",
            "     blocks.23.att.value.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.066 0.079 0.070 0.058 0.058 0.056 0.054 0.053 0.053 0.066 \n",
            "blocks.23.att.receptance.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "    blocks.23.att.output.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.076 0.061 0.060 0.061 0.062 0.065 0.067 0.080 0.071 0.059 0.057 0.055 0.053 0.053 0.053 0.066 \n",
            "       blocks.23.ffn.time_mix_k - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.23.ffn.time_mix_r - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.23.ffn.key.weight - [ 2560, 10240], type =   FP16 quantizing... size =    50.00 MB ->    18.75 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "blocks.23.ffn.receptance.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
            "     blocks.23.ffn.value.weight - [10240,  2560], type =   FP16 quantizing... size =    50.00 MB ->    18.75 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "           blocks.24.ln1.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "             blocks.24.ln1.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "           blocks.24.ln2.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "             blocks.24.ln2.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.24.att.time_decay - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.24.att.time_first - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.24.att.time_mix_k - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.24.att.time_mix_v - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.24.att.time_mix_r - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.24.att.key.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.074 0.061 0.061 0.062 0.063 0.064 0.065 0.078 0.070 0.058 0.058 0.057 0.056 0.055 0.054 0.066 \n",
            "     blocks.24.att.value.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.066 0.079 0.070 0.058 0.057 0.056 0.054 0.054 0.053 0.066 \n",
            "blocks.24.att.receptance.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "    blocks.24.att.output.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.076 0.061 0.060 0.061 0.062 0.065 0.066 0.080 0.071 0.059 0.057 0.055 0.054 0.053 0.053 0.066 \n",
            "       blocks.24.ffn.time_mix_k - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.24.ffn.time_mix_r - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.24.ffn.key.weight - [ 2560, 10240], type =   FP16 quantizing... size =    50.00 MB ->    18.75 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "blocks.24.ffn.receptance.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "     blocks.24.ffn.value.weight - [10240,  2560], type =   FP16 quantizing... size =    50.00 MB ->    18.75 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "           blocks.25.ln1.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "             blocks.25.ln1.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "           blocks.25.ln2.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "             blocks.25.ln2.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.25.att.time_decay - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.25.att.time_first - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.25.att.time_mix_k - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.25.att.time_mix_v - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.25.att.time_mix_r - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.25.att.key.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.074 0.061 0.061 0.062 0.063 0.064 0.065 0.078 0.070 0.058 0.058 0.057 0.056 0.055 0.054 0.066 \n",
            "     blocks.25.att.value.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.079 0.070 0.058 0.058 0.056 0.055 0.054 0.054 0.066 \n",
            "blocks.25.att.receptance.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.064 0.065 0.078 0.070 0.058 0.058 0.056 0.055 0.054 0.054 0.066 \n",
            "    blocks.25.att.output.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.061 0.061 0.063 0.065 0.066 0.080 0.071 0.059 0.057 0.055 0.054 0.053 0.053 0.066 \n",
            "       blocks.25.ffn.time_mix_k - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.25.ffn.time_mix_r - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.25.ffn.key.weight - [ 2560, 10240], type =   FP16 quantizing... size =    50.00 MB ->    18.75 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "blocks.25.ffn.receptance.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "     blocks.25.ffn.value.weight - [10240,  2560], type =   FP16 quantizing... size =    50.00 MB ->    18.75 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "           blocks.26.ln1.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "             blocks.26.ln1.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "           blocks.26.ln2.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "             blocks.26.ln2.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.26.att.time_decay - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.26.att.time_first - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.26.att.time_mix_k - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.26.att.time_mix_v - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.26.att.time_mix_r - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.26.att.key.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.074 0.061 0.061 0.062 0.063 0.064 0.065 0.077 0.070 0.058 0.058 0.057 0.056 0.055 0.054 0.066 \n",
            "     blocks.26.att.value.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.079 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "blocks.26.att.receptance.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.064 0.065 0.078 0.070 0.058 0.058 0.056 0.055 0.055 0.054 0.066 \n",
            "    blocks.26.att.output.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.061 0.061 0.063 0.065 0.066 0.079 0.070 0.059 0.057 0.056 0.054 0.053 0.054 0.066 \n",
            "       blocks.26.ffn.time_mix_k - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.26.ffn.time_mix_r - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.26.ffn.key.weight - [ 2560, 10240], type =   FP16 quantizing... size =    50.00 MB ->    18.75 MB | hist: 0.074 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "blocks.26.ffn.receptance.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
            "     blocks.26.ffn.value.weight - [10240,  2560], type =   FP16 quantizing... size =    50.00 MB ->    18.75 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "           blocks.27.ln1.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "             blocks.27.ln1.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "           blocks.27.ln2.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "             blocks.27.ln2.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.27.att.time_decay - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.27.att.time_first - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.27.att.time_mix_k - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.27.att.time_mix_v - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.27.att.time_mix_r - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.27.att.key.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.074 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.058 0.057 0.056 0.055 0.054 0.065 \n",
            "     blocks.27.att.value.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.053 0.066 \n",
            "blocks.27.att.receptance.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.074 0.061 0.061 0.062 0.063 0.064 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "    blocks.27.att.output.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.061 0.061 0.063 0.065 0.066 0.079 0.070 0.058 0.057 0.056 0.054 0.054 0.053 0.066 \n",
            "       blocks.27.ffn.time_mix_k - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.27.ffn.time_mix_r - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.27.ffn.key.weight - [ 2560, 10240], type =   FP16 quantizing... size =    50.00 MB ->    18.75 MB | hist: 0.074 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "blocks.27.ffn.receptance.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
            "     blocks.27.ffn.value.weight - [10240,  2560], type =   FP16 quantizing... size =    50.00 MB ->    18.75 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "           blocks.28.ln1.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "             blocks.28.ln1.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "           blocks.28.ln2.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "             blocks.28.ln2.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.28.att.time_decay - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.28.att.time_first - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.28.att.time_mix_k - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.28.att.time_mix_v - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.28.att.time_mix_r - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.28.att.key.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.074 0.061 0.061 0.062 0.063 0.064 0.065 0.078 0.070 0.058 0.058 0.057 0.056 0.055 0.054 0.065 \n",
            "     blocks.28.att.value.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.053 0.066 \n",
            "blocks.28.att.receptance.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.064 0.065 0.078 0.070 0.058 0.058 0.056 0.055 0.054 0.054 0.066 \n",
            "    blocks.28.att.output.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.066 0.079 0.070 0.058 0.057 0.056 0.054 0.053 0.054 0.066 \n",
            "       blocks.28.ffn.time_mix_k - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.28.ffn.time_mix_r - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.28.ffn.key.weight - [ 2560, 10240], type =   FP16 quantizing... size =    50.00 MB ->    18.75 MB | hist: 0.074 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "blocks.28.ffn.receptance.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.062 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.065 \n",
            "     blocks.28.ffn.value.weight - [10240,  2560], type =   FP16 quantizing... size =    50.00 MB ->    18.75 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "           blocks.29.ln1.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "             blocks.29.ln1.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "           blocks.29.ln2.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "             blocks.29.ln2.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.29.att.time_decay - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.29.att.time_first - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.29.att.time_mix_k - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.29.att.time_mix_v - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.29.att.time_mix_r - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.29.att.key.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.074 0.061 0.061 0.062 0.063 0.064 0.065 0.078 0.070 0.058 0.058 0.057 0.056 0.055 0.054 0.065 \n",
            "     blocks.29.att.value.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "blocks.29.att.receptance.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.074 0.061 0.061 0.062 0.063 0.064 0.065 0.078 0.070 0.058 0.058 0.057 0.055 0.055 0.054 0.066 \n",
            "    blocks.29.att.output.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.066 0.079 0.070 0.058 0.057 0.056 0.054 0.054 0.053 0.066 \n",
            "       blocks.29.ffn.time_mix_k - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.29.ffn.time_mix_r - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.29.ffn.key.weight - [ 2560, 10240], type =   FP16 quantizing... size =    50.00 MB ->    18.75 MB | hist: 0.074 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.058 0.056 0.055 0.054 0.054 0.066 \n",
            "blocks.29.ffn.receptance.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.074 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.058 0.056 0.055 0.054 0.054 0.065 \n",
            "     blocks.29.ffn.value.weight - [10240,  2560], type =   FP16 quantizing... size =    50.00 MB ->    18.75 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "           blocks.30.ln1.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "             blocks.30.ln1.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "           blocks.30.ln2.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "             blocks.30.ln2.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.30.att.time_decay - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.30.att.time_first - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.30.att.time_mix_k - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.30.att.time_mix_v - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.30.att.time_mix_r - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.30.att.key.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.074 0.061 0.061 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.058 0.057 0.055 0.055 0.053 0.065 \n",
            "     blocks.30.att.value.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.079 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "blocks.30.att.receptance.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.064 0.065 0.078 0.070 0.058 0.058 0.056 0.055 0.054 0.054 0.066 \n",
            "    blocks.30.att.output.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.061 0.061 0.063 0.065 0.066 0.079 0.070 0.058 0.058 0.056 0.054 0.054 0.054 0.066 \n",
            "       blocks.30.ffn.time_mix_k - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.30.ffn.time_mix_r - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.30.ffn.key.weight - [ 2560, 10240], type =   FP16 quantizing... size =    50.00 MB ->    18.75 MB | hist: 0.074 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "blocks.30.ffn.receptance.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.074 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "     blocks.30.ffn.value.weight - [10240,  2560], type =   FP16 quantizing... size =    50.00 MB ->    18.75 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "           blocks.31.ln1.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "             blocks.31.ln1.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "           blocks.31.ln2.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "             blocks.31.ln2.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.31.att.time_decay - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.31.att.time_first - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.31.att.time_mix_k - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.31.att.time_mix_v - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.31.att.time_mix_r - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.31.att.key.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.074 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.058 0.056 0.055 0.055 0.054 0.065 \n",
            "     blocks.31.att.value.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "blocks.31.att.receptance.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.074 0.061 0.061 0.062 0.064 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "    blocks.31.att.output.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.076 0.061 0.061 0.061 0.063 0.065 0.066 0.079 0.071 0.059 0.057 0.055 0.054 0.053 0.053 0.066 \n",
            "       blocks.31.ffn.time_mix_k - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.31.ffn.time_mix_r - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "       blocks.31.ffn.key.weight - [ 2560, 10240], type =   FP16 quantizing... size =    50.00 MB ->    18.75 MB | hist: 0.075 0.061 0.062 0.062 0.063 0.065 0.065 0.078 0.069 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "blocks.31.ffn.receptance.weight - [ 2560,  2560], type =   FP16 quantizing... size =    12.50 MB ->     4.69 MB | hist: 0.074 0.061 0.061 0.062 0.063 0.064 0.065 0.078 0.070 0.058 0.058 0.057 0.055 0.055 0.054 0.066 \n",
            "     blocks.31.ffn.value.weight - [10240,  2560], type =   FP16 quantizing... size =    50.00 MB ->    18.75 MB | hist: 0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 \n",
            "                  ln_out.weight - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "                    ln_out.bias - [ 2560,     1], type =   FP32 size =    0.010 MB\n",
            "                    head.weight - [ 2560, 65536], type =   FP16 size =  320.000 MB\n",
            "original size     =  5843.48 MB\n",
            "quantized size    =  2593.48 MB\n",
            "compression ratio =     2.25\n",
            "hist: \n",
            "Done\n",
            "0.075 0.061 0.061 0.062 0.063 0.065 0.065 0.078 0.070 0.058 0.057 0.056 0.055 0.054 0.054 0.066 "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat with model"
      ],
      "metadata": {
        "id": "9k_5Nc0L2RhW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python rwkv.cpp/python/chat_with_bot.py RWKV-4-World-3B-v1-20230619-ctx4096_Q5_1.bin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FpzQe0d1vjF",
        "outputId": "080584c0-002b-46b9-f919-9b42698a6f05"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "System info: AVX=1 AVX2=1 AVX512=0 FMA=1 NEON=0 ARM_FMA=0 F16C=1 FP16_VA=0 WASM_SIMD=0 BLAS=0 SSE3=1 VSX=0\n",
            "Loading RWKV model\n",
            "Loading World v20230424 tokenizer\n",
            "Processing 178 prompt tokens, may take a while\n",
            "Processed in 33 s, 189 ms per token\n",
            "\n",
            "Chat initialized! Your name is User. Write something and press Enter. Use \\n to add line breaks to your message.\n",
            "> User: Hva heter fargene i regnbue?\n",
            "> Bot: The colors in the rainbow are red, orange, yellow, green, blue, indigo, and violet.\n",
            "\n",
            "> User: \n",
            "> Bot: Hello! How can I assist you today?\n",
            "\n",
            "> User: Hva er 3+2?\n",
            "> Bot: 3+2 = 5.\n",
            "\n",
            "> User: Hva er 7*6+5*4?\n",
            "> Bot: 7*6+5*4 = 35.\n",
            "\n",
            "> User: How many days are there in three weeks?\n",
            "> Bot: There are 14 days in three weeks.\n",
            "\n",
            "> User: Write a poem about reindeers.\n",
            "> Bot: In the winter wonderland,\n",
            "The reindeers dance and play,\n",
            "Their antlers shining bright,\n",
            "As they frolic and play.\n",
            "The snowflakes flutter by,\n",
            "In a magical sight,\n",
            "As the sun sets in the sky,\n",
            "And the snowflakes light up the sky.\n",
            "\n",
            "> User: Traceback (most recent call last):\n",
            "  File \"/content/rwkv.cpp/python/chat_with_bot.py\", line 125, in <module>\n",
            "    user_input: str = input(f'> {user}{separator} ')\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}