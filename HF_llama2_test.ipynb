{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMIImw6KpqK4/cJ/ZhUEiHm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jankovicsandras/ml/blob/main/HF_llama2_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Init"
      ],
      "metadata": {
        "id": "yIfIqQEDPnXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! ls -la\n",
        "! pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhTEY8T3PmFU",
        "outputId": "715784bd-46ab-4691-e192-85c7c902cf8a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 5379436\n",
            "drwxr-xr-x 1 root root       4096 Aug 24 06:34 .\n",
            "drwxr-xr-x 1 root root       4096 Aug 24 06:10 ..\n",
            "drwxr-xr-x 4 root root       4096 Aug 22 13:38 .config\n",
            "-rw-r--r-- 1 root root 5508521088 Jul 18 18:05 llama-2-13b-chat.ggmlv3.q2_K.bin\n",
            "drwxr-xr-x 1 root root       4096 Aug 22 13:39 sample_data\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model download"
      ],
      "metadata": {
        "id": "337frCuWQC42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! wget https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML/resolve/main/llama-2-13b-chat.ggmlv3.q2_K.bin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NTifmRqP4Rr",
        "outputId": "81a7cf0f-29e4-48a7-b39b-4457ae137eae"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-24 06:34:25--  https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML/resolve/main/llama-2-13b-chat.ggmlv3.q2_K.bin\n",
            "Resolving huggingface.co (huggingface.co)... 13.35.166.50, 13.35.166.69, 13.35.166.36, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.35.166.50|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/cd/43/cd4356b11767f5136b31b27dbb8863d6dd69a4010e034ef75be9c2c12fcd10f7/de25498144f05fd3ee41cd2250c16f23a8415a4a4c9f4c1df1a3efd9b3c0991d?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama-2-13b-chat.ggmlv3.q2_K.bin%3B+filename%3D%22llama-2-13b-chat.ggmlv3.q2_K.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1693118065&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5MzExODA2NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9jZC80My9jZDQzNTZiMTE3NjdmNTEzNmIzMWIyN2RiYjg4NjNkNmRkNjlhNDAxMGUwMzRlZjc1YmU5YzJjMTJmY2QxMGY3L2RlMjU0OTgxNDRmMDVmZDNlZTQxY2QyMjUwYzE2ZjIzYTg0MTVhNGE0YzlmNGMxZGYxYTNlZmQ5YjNjMDk5MWQ%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=xrcVogb3LQYML5XK0YRnG3W8cmKDvsud-R7aIZdAR8DB07jxGFRly6BTo6hzptiX0b9m4rOKwI2HjTSqvUS2NV4PoYqzPNQPxzWc7bmxqfg5bMdJuDWBrh25U8YeNRO2FPpHMRaLuyuoq%7E1Zo4GUTd7X-JJ4BEPlNwdpoe9O7QKUBVJQMjCgO7pivG1b2c2VyL%7E%7EqJH0ho%7ELQXkpx%7EhRU4wg-VMSwAZr7wOcTW3GtCxAbz3yvo8cMb6pP9LnhAfOKrI0gbqirnzI9VRhrCVrd8OAGvdRpw-Kn1qhjmCTudH1JZZzNq0-KhUNt4IhZ7UEzoRjndFDbLL18GDgEQh2VA__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-08-24 06:34:25--  https://cdn-lfs.huggingface.co/repos/cd/43/cd4356b11767f5136b31b27dbb8863d6dd69a4010e034ef75be9c2c12fcd10f7/de25498144f05fd3ee41cd2250c16f23a8415a4a4c9f4c1df1a3efd9b3c0991d?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama-2-13b-chat.ggmlv3.q2_K.bin%3B+filename%3D%22llama-2-13b-chat.ggmlv3.q2_K.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1693118065&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5MzExODA2NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9jZC80My9jZDQzNTZiMTE3NjdmNTEzNmIzMWIyN2RiYjg4NjNkNmRkNjlhNDAxMGUwMzRlZjc1YmU5YzJjMTJmY2QxMGY3L2RlMjU0OTgxNDRmMDVmZDNlZTQxY2QyMjUwYzE2ZjIzYTg0MTVhNGE0YzlmNGMxZGYxYTNlZmQ5YjNjMDk5MWQ%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=xrcVogb3LQYML5XK0YRnG3W8cmKDvsud-R7aIZdAR8DB07jxGFRly6BTo6hzptiX0b9m4rOKwI2HjTSqvUS2NV4PoYqzPNQPxzWc7bmxqfg5bMdJuDWBrh25U8YeNRO2FPpHMRaLuyuoq%7E1Zo4GUTd7X-JJ4BEPlNwdpoe9O7QKUBVJQMjCgO7pivG1b2c2VyL%7E%7EqJH0ho%7ELQXkpx%7EhRU4wg-VMSwAZr7wOcTW3GtCxAbz3yvo8cMb6pP9LnhAfOKrI0gbqirnzI9VRhrCVrd8OAGvdRpw-Kn1qhjmCTudH1JZZzNq0-KhUNt4IhZ7UEzoRjndFDbLL18GDgEQh2VA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 13.35.7.14, 13.35.7.93, 13.35.7.113, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|13.35.7.14|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5508521088 (5.1G) [application/octet-stream]\n",
            "Saving to: ‘llama-2-13b-chat.ggmlv3.q2_K.bin’\n",
            "\n",
            "llama-2-13b-chat.gg 100%[===================>]   5.13G  20.0MB/s    in 4m 42s  \n",
            "\n",
            "2023-08-24 06:39:08 (18.7 MB/s) - ‘llama-2-13b-chat.ggmlv3.q2_K.bin’ saved [5508521088/5508521088]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install deps\n"
      ],
      "metadata": {
        "id": "b0jitre4aW-T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ofOv548DZ32W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2cd5524-60cc-468c-dea7-ef980ee59ad3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.7 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.5/1.7 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.7 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.6/45.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip -q install langchain transformers huggingface_hub accelerate bitsandbytes llama-cpp-python"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import, API token init"
      ],
      "metadata": {
        "id": "F5psBsU0bLYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "#from langchain import PromptTemplate, LLMChain, HuggingFaceHub\n",
        "from getpass import getpass\n",
        "HUGGINGFACEHUB_API_TOKEN = getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nubjpwNca_Kw",
        "outputId": "66ef083a-46d3-407c-90c3-59926e7c5e01"
      },
      "execution_count": 1,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Env"
      ],
      "metadata": {
        "id": "whPMZSDrbYqT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = HUGGINGFACEHUB_API_TOKEN"
      ],
      "metadata": {
        "id": "re4CQ44NbcHI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PromptTemplate"
      ],
      "metadata": {
        "id": "CXmg9qIQblfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate #, LLMChain, HuggingFaceHub\n",
        "\n",
        "template = \"\"\"Question: {question}\n",
        "Answer: Let's think step by step. \"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "        template=template,\n",
        "    input_variables=['question']\n",
        ")"
      ],
      "metadata": {
        "id": "lqaddfrYcAyU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model"
      ],
      "metadata": {
        "id": "dXKRrjnpc7rJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import HuggingFaceHub, LLMChain, LlamaCpp\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaTokenizer\n",
        "\n",
        "model_name = \"llama-2-13b-chat.ggmlv3.q2_K.bin\"\n",
        "llm = LlamaCpp(\n",
        "\tmodel_path=model_name #, model_kwargs={\"temperature\":0.1, \"max_length\":128}\n",
        ")\n",
        "\n",
        "#m = AutoModelForCausalLM.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "id": "U1qiKtBigD4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b324c925-3a7a-4926-d077-092f250c9be7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LangChain LLMChain run"
      ],
      "metadata": {
        "id": "ZfaHJCQYifoV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "#question = \"What is the number of the days of the week squared?\"  # correct first try from llama-2-13b-chat.ggmlv3.q2_K.bin\n",
        "#question = \"How many fingers are there on six hands?\" # correct first try from llama-2-13b-chat.ggmlv3.q2_K.bin\n",
        "question = \"\"\"Andy has four apples. Bill has seven oranges. Claire has three apples.\n",
        "Daniel has two oranges. Sort the people by the number of their fruits after Andy gives Claire two and Bill gives Daniel four.\"\"\" # wrong first try from llama-2-13b-chat.ggmlv3.q2_K.bin\n",
        "# A=4 B=7 C=3 D=2 ; A-2 C+2 B-4 D+4 ; A=2 B=3 C=5 D=6 ; ABCD or DCBA is correct\n",
        "\n",
        "result = llm_chain(question)\n",
        "\n",
        "print(result['text'])\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akDv5vrpii4J",
        "outputId": "f79e8bae-0ef9-4fb2-96d4-989b28919a4c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "First, Andy gives Claire two apples, so she now has two + three = five apples.\n",
            "Next, Bill gives Daniel four oranges, so Daniel now has two + four = six oranges. \n",
            "Now, let's compare the total number of fruits for each person: \n",
            "Claire - five apples \n",
            "Bill - six oranges \n",
            "Daniel - six oranges and two apples (Andy gave him two apples)  \n",
            "So, the order from highest to lowest is:\n",
            "Claire, Bill, Daniel.\n",
            "{'question': 'Andy has four apples. Bill has seven oranges. Claire has three apples. \\nDaniel has two oranges. Sort the people by the number of their fruits after Andy gives Claire two and Bill gives Daniel four.', 'text': \"\\nFirst, Andy gives Claire two apples, so she now has two + three = five apples.\\nNext, Bill gives Daniel four oranges, so Daniel now has two + four = six oranges. \\nNow, let's compare the total number of fruits for each person: \\nClaire - five apples \\nBill - six oranges \\nDaniel - six oranges and two apples (Andy gave him two apples)  \\nSo, the order from highest to lowest is:\\nClaire, Bill, Daniel.\"}\n"
          ]
        }
      ]
    }
  ]
}